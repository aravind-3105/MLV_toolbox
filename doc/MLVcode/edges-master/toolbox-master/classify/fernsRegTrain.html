<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of fernsRegTrain</title>
  <meta name="keywords" content="fernsRegTrain">
  <meta name="description" content="Train boosted fern regressor.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../../index.html">Home</a> &gt;  <a href="../../../index.html">MLVcode</a> &gt; <a href="../../index.html">edges-master</a> &gt; <a href="#">toolbox-master</a> &gt; <a href="index.html">classify</a> &gt; fernsRegTrain.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../../index.html"><img alt="<" border="0" src="../../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for MLVcode\edges-master\toolbox-master\classify&nbsp;<img alt=">" border="0" src="../../../../right.png"></a></td></tr></table>-->

<h1>fernsRegTrain
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>Train boosted fern regressor.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>function [ferns,ysPr] = fernsRegTrain( data, ys, varargin ) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Train boosted fern regressor.

 Boosted regression using random ferns as the weak regressor. See &quot;Greedy
 function approximation: A gradient boosting machine&quot;, Friedman, Annals of
 Statistics 2001, for more details on boosted regression.

 A few notes on the parameters: 'type' should in general be set to 'res'
 (the 'ave' version is an undocumented variant that only performs well
 under limited conditions). 'loss' determines the loss function being
 optimized, in general the 'L2' version is the most robust and effective.
 'reg' is a regularization term for the ferns, a low value such as .01 can
 improve results. Setting the learning rate 'eta' is crucial in order to
 achieve good performance, especially on noisy data. In general, eta
 should decreased as M is increased.

 Dimensions:
  M - number ferns
  R - number repeats
  S - fern depth
  N - number samples
  F - number features

 USAGE
  [ferns,ysPr] = fernsRegTrain( data, hs, [varargin] )

 INPUTS
  data     - [NxF] N length F feature vectors
  ys       - [Nx1] target output values
  varargin - additional params (struct or name/value pairs)
   .type     - ['res'] options include {'res','ave'}
   .loss     - ['L2'] options include {'L1','L2','exp'}
   .S        - [2] fern depth (ferns are exponential in S)
   .M        - [50] number ferns (same as number phases)
   .R        - [10] number repetitions per fern
   .thrr     - [0 1] range for randomly generated thresholds
   .reg      - [0.01] fern regularization term in [0,1]
   .eta      - [1] learning rate in [0,1] (not used if type='ave')
   .verbose  - [0] if true output info to display

 OUTPUTS
  ferns    - learned fern model w the following fields
   .fids     - [MxS] feature ids for each fern for each depth
   .thrs     - [MxS] threshold corresponding to each fid
   .ysFern   - [2^SxM] stored values at fern leaves
   .loss     - loss(ys,ysGt) computes loss of ys relateive to ysGt
  ysPr     - [Nx1] predicted output values

 EXAMPLE
  %% generate toy data
  N=1000; sig=.5; f=@(x) cos(x*pi*4)+(x+1).^2;
  xs0=rand(N,1); ys0=f(xs0)+randn(N,1)*sig;
  xs1=rand(N,1); ys1=f(xs1)+randn(N,1)*sig;
  %% train and apply fern regressor
  prm=struct('type','res','loss','L2','eta',.05,...
    'thrr',[-1 1],'reg',.01,'S',2,'M',1000,'R',3,'verbose',0);
  tic, [ferns,ysPr0] = fernsRegTrain(xs0,ys0,prm); toc
  tic, ysPr1 = fernsRegApply( xs1, ferns ); toc
  fprintf('errors train=%f test=%f\n',...
    ferns.loss(ysPr0,ys0),ferns.loss(ysPr1,ys1));
  %% visualize results
  figure(1); clf; hold on; plot(xs0,ys0,'.b'); plot(xs0,ysPr0,'.r');
  figure(2); clf; hold on; plot(xs1,ys1,'.b'); plot(xs1,ysPr1,'.r');

 See also <a href="fernsRegApply.html" class="code" title="function [ys,ysCum] = fernsRegApply( data, ferns, inds )">fernsRegApply</a>, <a href="fernsInds.html" class="code" title="function inds = fernsInds( data, fids, thrs )">fernsInds</a>

 Piotr's Computer Vision Matlab Toolbox      Version 2.50
 Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]
 Licensed under the Simplified BSD License [see external/bsd.txt]</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
<li><a href="fernsInds.html" class="code" title="function inds = fernsInds( data, fids, thrs )">fernsInds</a>	Compute indices for each input by each fern.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [fids,thrs,ysFern,ysPr] = trainFern( data, ys, S, thrr, reg )</a></li><li><a href="#_sub2" class="code">function m = medianw(x,w)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [ferns,ysPr] = fernsRegTrain( data, ys, varargin )</a>
0002 <span class="comment">% Train boosted fern regressor.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Boosted regression using random ferns as the weak regressor. See &quot;Greedy</span>
0005 <span class="comment">% function approximation: A gradient boosting machine&quot;, Friedman, Annals of</span>
0006 <span class="comment">% Statistics 2001, for more details on boosted regression.</span>
0007 <span class="comment">%</span>
0008 <span class="comment">% A few notes on the parameters: 'type' should in general be set to 'res'</span>
0009 <span class="comment">% (the 'ave' version is an undocumented variant that only performs well</span>
0010 <span class="comment">% under limited conditions). 'loss' determines the loss function being</span>
0011 <span class="comment">% optimized, in general the 'L2' version is the most robust and effective.</span>
0012 <span class="comment">% 'reg' is a regularization term for the ferns, a low value such as .01 can</span>
0013 <span class="comment">% improve results. Setting the learning rate 'eta' is crucial in order to</span>
0014 <span class="comment">% achieve good performance, especially on noisy data. In general, eta</span>
0015 <span class="comment">% should decreased as M is increased.</span>
0016 <span class="comment">%</span>
0017 <span class="comment">% Dimensions:</span>
0018 <span class="comment">%  M - number ferns</span>
0019 <span class="comment">%  R - number repeats</span>
0020 <span class="comment">%  S - fern depth</span>
0021 <span class="comment">%  N - number samples</span>
0022 <span class="comment">%  F - number features</span>
0023 <span class="comment">%</span>
0024 <span class="comment">% USAGE</span>
0025 <span class="comment">%  [ferns,ysPr] = fernsRegTrain( data, hs, [varargin] )</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% INPUTS</span>
0028 <span class="comment">%  data     - [NxF] N length F feature vectors</span>
0029 <span class="comment">%  ys       - [Nx1] target output values</span>
0030 <span class="comment">%  varargin - additional params (struct or name/value pairs)</span>
0031 <span class="comment">%   .type     - ['res'] options include {'res','ave'}</span>
0032 <span class="comment">%   .loss     - ['L2'] options include {'L1','L2','exp'}</span>
0033 <span class="comment">%   .S        - [2] fern depth (ferns are exponential in S)</span>
0034 <span class="comment">%   .M        - [50] number ferns (same as number phases)</span>
0035 <span class="comment">%   .R        - [10] number repetitions per fern</span>
0036 <span class="comment">%   .thrr     - [0 1] range for randomly generated thresholds</span>
0037 <span class="comment">%   .reg      - [0.01] fern regularization term in [0,1]</span>
0038 <span class="comment">%   .eta      - [1] learning rate in [0,1] (not used if type='ave')</span>
0039 <span class="comment">%   .verbose  - [0] if true output info to display</span>
0040 <span class="comment">%</span>
0041 <span class="comment">% OUTPUTS</span>
0042 <span class="comment">%  ferns    - learned fern model w the following fields</span>
0043 <span class="comment">%   .fids     - [MxS] feature ids for each fern for each depth</span>
0044 <span class="comment">%   .thrs     - [MxS] threshold corresponding to each fid</span>
0045 <span class="comment">%   .ysFern   - [2^SxM] stored values at fern leaves</span>
0046 <span class="comment">%   .loss     - loss(ys,ysGt) computes loss of ys relateive to ysGt</span>
0047 <span class="comment">%  ysPr     - [Nx1] predicted output values</span>
0048 <span class="comment">%</span>
0049 <span class="comment">% EXAMPLE</span>
0050 <span class="comment">%  %% generate toy data</span>
0051 <span class="comment">%  N=1000; sig=.5; f=@(x) cos(x*pi*4)+(x+1).^2;</span>
0052 <span class="comment">%  xs0=rand(N,1); ys0=f(xs0)+randn(N,1)*sig;</span>
0053 <span class="comment">%  xs1=rand(N,1); ys1=f(xs1)+randn(N,1)*sig;</span>
0054 <span class="comment">%  %% train and apply fern regressor</span>
0055 <span class="comment">%  prm=struct('type','res','loss','L2','eta',.05,...</span>
0056 <span class="comment">%    'thrr',[-1 1],'reg',.01,'S',2,'M',1000,'R',3,'verbose',0);</span>
0057 <span class="comment">%  tic, [ferns,ysPr0] = fernsRegTrain(xs0,ys0,prm); toc</span>
0058 <span class="comment">%  tic, ysPr1 = fernsRegApply( xs1, ferns ); toc</span>
0059 <span class="comment">%  fprintf('errors train=%f test=%f\n',...</span>
0060 <span class="comment">%    ferns.loss(ysPr0,ys0),ferns.loss(ysPr1,ys1));</span>
0061 <span class="comment">%  %% visualize results</span>
0062 <span class="comment">%  figure(1); clf; hold on; plot(xs0,ys0,'.b'); plot(xs0,ysPr0,'.r');</span>
0063 <span class="comment">%  figure(2); clf; hold on; plot(xs1,ys1,'.b'); plot(xs1,ysPr1,'.r');</span>
0064 <span class="comment">%</span>
0065 <span class="comment">% See also fernsRegApply, fernsInds</span>
0066 <span class="comment">%</span>
0067 <span class="comment">% Piotr's Computer Vision Matlab Toolbox      Version 2.50</span>
0068 <span class="comment">% Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]</span>
0069 <span class="comment">% Licensed under the Simplified BSD License [see external/bsd.txt]</span>
0070 
0071 <span class="comment">% get/check parameters</span>
0072 dfs={<span class="string">'type'</span>,<span class="string">'res'</span>,<span class="string">'loss'</span>,<span class="string">'L2'</span>,<span class="string">'S'</span>,2,<span class="string">'M'</span>,50,<span class="string">'R'</span>,10,<span class="string">'thrr'</span>,[0 1],<span class="keyword">...</span>
0073   <span class="string">'reg'</span>,0.01,<span class="string">'eta'</span>,1,<span class="string">'verbose'</span>,0};
0074 [type,loss,S,M,R,thrr,reg,eta,verbose]=getPrmDflt(varargin,dfs,1);
0075 type=type(1:3); assert(any(strcmp(type,{<span class="string">'res'</span>,<span class="string">'ave'</span>})));
0076 assert(any(strcmp(loss,{<span class="string">'L1'</span>,<span class="string">'L2'</span>,<span class="string">'exp'</span>}))); N=length(ys);
0077 <span class="keyword">if</span>(strcmp(type,<span class="string">'ave'</span>)), eta=1; <span class="keyword">end</span>
0078 <span class="comment">% train stagewise regressor (residual or average)</span>
0079 fids=zeros(M,S,<span class="string">'uint32'</span>); thrs=zeros(M,S);
0080 ysSum=zeros(N,1); ysFern=zeros(2^S,M);
0081 <span class="keyword">for</span> m=1:M
0082   <span class="comment">% train R random ferns using different losses, keep best</span>
0083   <span class="keyword">if</span>(strcmp(type,<span class="string">'ave'</span>)), d=m; <span class="keyword">else</span> d=1; <span class="keyword">end</span>
0084   ysTar=d*ys-ysSum; best={};
0085   <span class="keyword">if</span>(strcmp(loss,<span class="string">'L1'</span>)), e=sum(abs(ysTar));
0086     <span class="keyword">for</span> r=1:R
0087       [fids1,thrs1,ysFern1,ys1]=<a href="#_sub1" class="code" title="subfunction [fids,thrs,ysFern,ysPr] = trainFern( data, ys, S, thrr, reg )">trainFern</a>(data,sign(ysTar),S,thrr,reg);
0088       a=<a href="#_sub2" class="code" title="subfunction m = medianw(x,w)">medianw</a>(ysTar./ys1,abs(ys1)); ysFern1=ysFern1*a; ys1=ys1*a;
0089       e1=sum(abs(ysTar-ys1));
0090       <span class="keyword">if</span>(e1&lt;=e), e=e1; best={fids1,thrs1,ysFern1,ys1}; <span class="keyword">end</span>
0091     <span class="keyword">end</span>
0092   <span class="keyword">elseif</span>(strcmp(loss,<span class="string">'L2'</span>)), e=sum(ysTar.^2);
0093     <span class="keyword">for</span> r=1:R
0094       [fids1,thrs1,ysFern1,ys1]=<a href="#_sub1" class="code" title="subfunction [fids,thrs,ysFern,ysPr] = trainFern( data, ys, S, thrr, reg )">trainFern</a>(data,ysTar,S,thrr,reg);
0095       e1=sum((ysTar-ys1).^2);
0096       <span class="keyword">if</span>(e1&lt;=e), e=e1; best={fids1,thrs1,ysFern1,ys1}; <span class="keyword">end</span>
0097     <span class="keyword">end</span>
0098   <span class="keyword">elseif</span>(strcmp(loss,<span class="string">'exp'</span>)), e=sum(exp(ysTar/d)+exp(-ysTar/d));
0099     ysDeriv=exp(ysTar/d)-exp(-ysTar/d);
0100     <span class="keyword">for</span> r=1:R
0101       [fids1,thrs1,ysFern1,ys1]=<a href="#_sub1" class="code" title="subfunction [fids,thrs,ysFern,ysPr] = trainFern( data, ys, S, thrr, reg )">trainFern</a>(data,ysDeriv,S,thrr,reg);
0102       e1=inf; <span class="keyword">if</span>(m==1), aBst=1; <span class="keyword">end</span>; aMin=aBst/5; aMax=aBst*5;
0103       <span class="keyword">for</span> phase=1:3, aDel=(aMax-aMin)/10;
0104         <span class="keyword">for</span> a=aMin:aDel:aMax
0105           eTmp=sum(exp((ysTar-a*ys1)/d)+exp((a*ys1-ysTar)/d));
0106           <span class="keyword">if</span>(eTmp&lt;e1), a1=a; e1=eTmp; <span class="keyword">end</span>
0107         <span class="keyword">end</span>; aMin=a1-aDel; aMax=a1+aDel;
0108       <span class="keyword">end</span>; ysFern1=ysFern1*a1; ys1=ys1*a1;
0109       <span class="keyword">if</span>(e1&lt;=e), e=e1; aBst=a1; best={fids1,thrs1,ysFern1,ys1}; <span class="keyword">end</span>
0110     <span class="keyword">end</span>
0111   <span class="keyword">end</span>
0112   <span class="comment">% store results and update sums</span>
0113   assert(~isempty(best)); [fids1,thrs1,ysFern1,ys1]=deal(best{:});
0114   fids(m,:)=fids1; thrs(m,:)=thrs1;
0115   ysFern(:,m)=ysFern1*eta; ysSum=ysSum+ys1*eta;
0116   <span class="keyword">if</span>(verbose), fprintf(<span class="string">'phase=%i  error=%f\n'</span>,m,e); <span class="keyword">end</span>
0117 <span class="keyword">end</span>
0118 <span class="comment">% create output struct</span>
0119 <span class="keyword">if</span>(strcmp(type,<span class="string">'ave'</span>)), d=M; <span class="keyword">else</span> d=1; <span class="keyword">end</span>; clear data;
0120 ferns=struct(<span class="string">'fids'</span>,fids,<span class="string">'thrs'</span>,thrs,<span class="string">'ysFern'</span>,ysFern/d); ysPr=ysSum/d;
0121 <span class="keyword">switch</span> loss
0122   <span class="keyword">case</span> <span class="string">'L1'</span>,  ferns.loss=@(ys,ysGt) mean(abs(ys-ysGt));
0123   <span class="keyword">case</span> <span class="string">'L2'</span>,  ferns.loss=@(ys,ysGt) mean((ys-ysGt).^2);
0124   <span class="keyword">case</span> <span class="string">'exp'</span>, ferns.loss=@(ys,ysGt) mean(exp(ys-ysGt)+exp(ysGt-ys))-2;
0125 <span class="keyword">end</span>
0126 <span class="keyword">end</span>
0127 
0128 <a name="_sub1" href="#_subfunctions" class="code">function [fids,thrs,ysFern,ysPr] = trainFern( data, ys, S, thrr, reg )</a>
0129 <span class="comment">% Train single random fern regressor.</span>
0130 [N,F]=size(data); mu=sum(ys)/N; ys=ys-mu;
0131 fids = uint32(floor(rand(1,S)*F+1));
0132 thrs = rand(1,S)*(thrr(2)-thrr(1))+thrr(1);
0133 inds = <a href="fernsInds.html" class="code" title="function inds = fernsInds( data, fids, thrs )">fernsInds</a>(data,fids,thrs);
0134 ysFern=zeros(2^S,1); cnts=zeros(2^S,1);
0135 <span class="keyword">for</span> n=1:N, ind=inds(n);
0136   ysFern(ind)=ysFern(ind)+ys(n);
0137   cnts(ind)=cnts(ind)+1;
0138 <span class="keyword">end</span>
0139 ysFern = ysFern ./ max(cnts+reg*N,eps) + mu;
0140 ysPr = ysFern(inds);
0141 <span class="keyword">end</span>
0142 
0143 <a name="_sub2" href="#_subfunctions" class="code">function m = medianw(x,w)</a>
0144 <span class="comment">% Compute weighted median of x.</span>
0145 [x,ord]=sort(x(:)); w=w(ord);
0146 [~,ind]=max(cumsum(w)&gt;=sum(w)/2);
0147 m = x(ind);
0148 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 05-May-2022 15:20:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>