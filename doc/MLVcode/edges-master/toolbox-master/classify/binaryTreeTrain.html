<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of binaryTreeTrain</title>
  <meta name="keywords" content="binaryTreeTrain">
  <meta name="description" content="Train binary decision tree classifier.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../../index.html">Home</a> &gt;  <a href="../../../index.html">MLVcode</a> &gt; <a href="../../index.html">edges-master</a> &gt; <a href="#">toolbox-master</a> &gt; <a href="index.html">classify</a> &gt; binaryTreeTrain.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../../index.html"><img alt="<" border="0" src="../../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for MLVcode\edges-master\toolbox-master\classify&nbsp;<img alt=">" border="0" src="../../../../right.png"></a></td></tr></table>-->

<h1>binaryTreeTrain
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>Train binary decision tree classifier.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>function [tree,data,err] = binaryTreeTrain( data, varargin ) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Train binary decision tree classifier.

 Highly optimized code for training decision trees over binary variables.
 Training a decision stump (depth=1) over 5000 features and 10000 training
 examples takes 70ms on a single core machine and *7ms* with 12 cores and
 OpenMP enabled (OpenMP is enabled by default, see toolboxCompile). This
 code shares similarities with forestTrain.m but is optimized for binary
 labels. Moreover, while forestTrain is meant for training random decision
 forests, this code is tuned for use with boosting (see adaBoostTrain.m).

 For more information on how to quickly boost decision trees see:
   [1] R. Appel, T. Fuchs, P. Dollár, P. Perona; &quot;Quickly Boosting
   Decision Trees – Pruning Underachieving Features Early,&quot; ICML 2013.
 The code here implements a simple brute-force strategy with the option to
 sample features used for training each node for additional speedups.
 Further gains using the ideas from the ICML paper are possible. If you
 use this code please consider citing our ICML paper.

 During training each feature is quantized to lie between [0,nBins-1],
 where nBins&lt;=256. Quantization is expensive and should be performed just
 once if training multiple trees. Note that the second output of the
 algorithm is the quantized data, this can be reused in future training.

 USAGE
  [tree,data,err] = binaryTreeTrain( data, [pTree] )

 INPUTS
  data       - data for training tree
   .X0         - [N0xF] negative feature vectors
   .X1         - [N1xF] positive feature vectors
   .wts0       - [N0x1] negative weights
   .wts1       - [N1x1] positive weights
   .xMin       - [1xF] optional vals defining feature quantization
   .xStep      - [1xF] optional vals defining feature quantization
   .xType      - [] optional original data type for features
  pTree      - additional params (struct or name/value pairs)
   .nBins      - [256] maximum number of quanizaton bins (&lt;=256)
   .maxDepth   - [1] maximum depth of tree
   .minWeight  - [.01] minimum sample weigth to allow split
   .fracFtrs   - [1] fraction of features to sample for each node split
   .nThreads   - [16] max number of computational threads to use

 OUTPUTS
  tree       - learned decision tree model struct w the following fields
   .fids       - [Kx1] feature ids for each node
   .thrs       - [Kx1] threshold corresponding to each fid
   .child      - [Kx1] index of child for each node (1-indexed)
   .hs         - [Kx1] log ratio (.5*log(p/(1-p)) at each node
   .weights    - [Kx1] total sample weight at each node
   .depth      - [Kx1] depth of each node
  data       - data used for training tree (quantized version of input)
  err        - decision tree training error

 EXAMPLE

 See also <a href="binaryTreeApply.html" class="code" title="function hs = binaryTreeApply( X, tree, maxDepth, minWeight, nThreads )">binaryTreeApply</a>, <a href="adaBoostTrain.html" class="code" title="function model = adaBoostTrain( X0, X1, varargin )">adaBoostTrain</a>, <a href="forestTrain.html" class="code" title="function forest = forestTrain( data, hs, varargin )">forestTrain</a>

 Piotr's Computer Vision Matlab Toolbox      Version 3.40
 Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]
 Licensed under the Simplified BSD License [see external/bsd.txt]</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
<li><a href="adaBoostTrain.html" class="code" title="function model = adaBoostTrain( X0, X1, varargin )">adaBoostTrain</a>	Train boosted decision tree classifier.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [tree,data,err] = binaryTreeTrain( data, varargin )</a>
0002 <span class="comment">% Train binary decision tree classifier.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Highly optimized code for training decision trees over binary variables.</span>
0005 <span class="comment">% Training a decision stump (depth=1) over 5000 features and 10000 training</span>
0006 <span class="comment">% examples takes 70ms on a single core machine and *7ms* with 12 cores and</span>
0007 <span class="comment">% OpenMP enabled (OpenMP is enabled by default, see toolboxCompile). This</span>
0008 <span class="comment">% code shares similarities with forestTrain.m but is optimized for binary</span>
0009 <span class="comment">% labels. Moreover, while forestTrain is meant for training random decision</span>
0010 <span class="comment">% forests, this code is tuned for use with boosting (see adaBoostTrain.m).</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% For more information on how to quickly boost decision trees see:</span>
0013 <span class="comment">%   [1] R. Appel, T. Fuchs, P. Dollár, P. Perona; &quot;Quickly Boosting</span>
0014 <span class="comment">%   Decision Trees – Pruning Underachieving Features Early,&quot; ICML 2013.</span>
0015 <span class="comment">% The code here implements a simple brute-force strategy with the option to</span>
0016 <span class="comment">% sample features used for training each node for additional speedups.</span>
0017 <span class="comment">% Further gains using the ideas from the ICML paper are possible. If you</span>
0018 <span class="comment">% use this code please consider citing our ICML paper.</span>
0019 <span class="comment">%</span>
0020 <span class="comment">% During training each feature is quantized to lie between [0,nBins-1],</span>
0021 <span class="comment">% where nBins&lt;=256. Quantization is expensive and should be performed just</span>
0022 <span class="comment">% once if training multiple trees. Note that the second output of the</span>
0023 <span class="comment">% algorithm is the quantized data, this can be reused in future training.</span>
0024 <span class="comment">%</span>
0025 <span class="comment">% USAGE</span>
0026 <span class="comment">%  [tree,data,err] = binaryTreeTrain( data, [pTree] )</span>
0027 <span class="comment">%</span>
0028 <span class="comment">% INPUTS</span>
0029 <span class="comment">%  data       - data for training tree</span>
0030 <span class="comment">%   .X0         - [N0xF] negative feature vectors</span>
0031 <span class="comment">%   .X1         - [N1xF] positive feature vectors</span>
0032 <span class="comment">%   .wts0       - [N0x1] negative weights</span>
0033 <span class="comment">%   .wts1       - [N1x1] positive weights</span>
0034 <span class="comment">%   .xMin       - [1xF] optional vals defining feature quantization</span>
0035 <span class="comment">%   .xStep      - [1xF] optional vals defining feature quantization</span>
0036 <span class="comment">%   .xType      - [] optional original data type for features</span>
0037 <span class="comment">%  pTree      - additional params (struct or name/value pairs)</span>
0038 <span class="comment">%   .nBins      - [256] maximum number of quanizaton bins (&lt;=256)</span>
0039 <span class="comment">%   .maxDepth   - [1] maximum depth of tree</span>
0040 <span class="comment">%   .minWeight  - [.01] minimum sample weigth to allow split</span>
0041 <span class="comment">%   .fracFtrs   - [1] fraction of features to sample for each node split</span>
0042 <span class="comment">%   .nThreads   - [16] max number of computational threads to use</span>
0043 <span class="comment">%</span>
0044 <span class="comment">% OUTPUTS</span>
0045 <span class="comment">%  tree       - learned decision tree model struct w the following fields</span>
0046 <span class="comment">%   .fids       - [Kx1] feature ids for each node</span>
0047 <span class="comment">%   .thrs       - [Kx1] threshold corresponding to each fid</span>
0048 <span class="comment">%   .child      - [Kx1] index of child for each node (1-indexed)</span>
0049 <span class="comment">%   .hs         - [Kx1] log ratio (.5*log(p/(1-p)) at each node</span>
0050 <span class="comment">%   .weights    - [Kx1] total sample weight at each node</span>
0051 <span class="comment">%   .depth      - [Kx1] depth of each node</span>
0052 <span class="comment">%  data       - data used for training tree (quantized version of input)</span>
0053 <span class="comment">%  err        - decision tree training error</span>
0054 <span class="comment">%</span>
0055 <span class="comment">% EXAMPLE</span>
0056 <span class="comment">%</span>
0057 <span class="comment">% See also binaryTreeApply, adaBoostTrain, forestTrain</span>
0058 <span class="comment">%</span>
0059 <span class="comment">% Piotr's Computer Vision Matlab Toolbox      Version 3.40</span>
0060 <span class="comment">% Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]</span>
0061 <span class="comment">% Licensed under the Simplified BSD License [see external/bsd.txt]</span>
0062 
0063 <span class="comment">% get parameters</span>
0064 dfs={<span class="string">'nBins'</span>,256,<span class="string">'maxDepth'</span>,1,<span class="string">'minWeight'</span>,.01,<span class="string">'fracFtrs'</span>,1,<span class="string">'nThreads'</span>,16};
0065 [nBins,maxDepth,minWeight,fracFtrs,nThreads]=getPrmDflt(varargin,dfs,1);
0066 assert(nBins&lt;=256);
0067 
0068 <span class="comment">% get data and normalize weights</span>
0069 dfs={ <span class="string">'X0'</span>,<span class="string">'REQ'</span>, <span class="string">'X1'</span>,<span class="string">'REQ'</span>, <span class="string">'wts0'</span>,[], <span class="string">'wts1'</span>,[], <span class="keyword">...</span>
0070   <span class="string">'xMin'</span>,[], <span class="string">'xStep'</span>,[], <span class="string">'xType'</span>,[] };
0071 [X0,X1,wts0,wts1,xMin,xStep,xType]=getPrmDflt(data,dfs,1);
0072 [N0,F]=size(X0); [N1,F1]=size(X1); assert(F==F1);
0073 <span class="keyword">if</span>(isempty(xType)), xMin=zeros(1,F); xStep=ones(1,F); xType=class(X0); <span class="keyword">end</span>
0074 assert(isfloat(wts0)); <span class="keyword">if</span>(isempty(wts0)), wts0=ones(N0,1)/N0; <span class="keyword">end</span>
0075 assert(isfloat(wts1)); <span class="keyword">if</span>(isempty(wts1)), wts1=ones(N1,1)/N1; <span class="keyword">end</span>
0076 w=sum(wts0)+sum(wts1); <span class="keyword">if</span>(abs(w-1)&gt;1e-3), wts0=wts0/w; wts1=wts1/w; <span class="keyword">end</span>
0077 
0078 <span class="comment">% quantize data to be between [0,nBins-1] if not already quantized</span>
0079 <span class="keyword">if</span>( ~isa(X0,<span class="string">'uint8'</span>) || ~isa(X1,<span class="string">'uint8'</span>) )
0080   xMin = min(min(X0),min(X1))-.01;
0081   xMax = max(max(X0),max(X1))+.01;
0082   xStep = (xMax-xMin) / (nBins-1);
0083   X0 = uint8(bsxfun(@times,bsxfun(@minus,X0,xMin),1./xStep));
0084   X1 = uint8(bsxfun(@times,bsxfun(@minus,X1,xMin),1./xStep));
0085 <span class="keyword">end</span>
0086 data=struct( <span class="string">'X0'</span>,X0, <span class="string">'X1'</span>,X1, <span class="string">'wts0'</span>,wts0, <span class="string">'wts1'</span>,wts1, <span class="keyword">...</span>
0087   <span class="string">'xMin'</span>,xMin, <span class="string">'xStep'</span>,xStep, <span class="string">'xType'</span>,xType );
0088 
0089 <span class="comment">% train decision tree classifier</span>
0090 K=2*(N0+N1); thrs=zeros(K,1,xType);
0091 hs=zeros(K,1,<span class="string">'single'</span>); weights=hs; errs=hs;
0092 fids=zeros(K,1,<span class="string">'uint32'</span>); child=fids; depth=fids;
0093 wtsAll0=cell(K,1); wtsAll0{1}=wts0;
0094 wtsAll1=cell(K,1); wtsAll1{1}=wts1; k=1; K=2;
0095 <span class="keyword">while</span>( k &lt; K )
0096   <span class="comment">% get node weights and prior</span>
0097   wts0=wtsAll0{k}; wtsAll0{k}=[]; w0=sum(wts0);
0098   wts1=wtsAll1{k}; wtsAll1{k}=[]; w1=sum(wts1);
0099   w=w0+w1; prior=w1/w; weights(k)=w; errs(k)=min(prior,1-prior);
0100   hs(k)=max(-4,min(4,.5*log(prior/(1-prior))));
0101   <span class="comment">% if nearly pure node or insufficient data don't train split</span>
0102   <span class="keyword">if</span>( prior&lt;1e-3||prior&gt;1-1e-3||depth(k)&gt;=maxDepth||w&lt;minWeight )
0103     k=k+1; <span class="keyword">continue</span>; <span class="keyword">end</span>
0104   <span class="comment">% train best stump</span>
0105   fidsSt=1:F; <span class="keyword">if</span>(fracFtrs&lt;1), fidsSt=randperm(F,floor(F*fracFtrs)); <span class="keyword">end</span>
0106   [errsSt,thrsSt] = binaryTreeTrain1(X0,X1,single(wts0/w),<span class="keyword">...</span>
0107     single(wts1/w),nBins,prior,uint32(fidsSt-1),nThreads);
0108   [~,fid]=min(errsSt); thr=single(thrsSt(fid))+.5; fid=fidsSt(fid);
0109   <span class="comment">% split data and continue</span>
0110   left0=X0(:,fid)&lt;thr; left1=X1(:,fid)&lt;thr;
0111   <span class="keyword">if</span>( (any(left0)||any(left1)) &amp;&amp; (any(~left0)||any(~left1)) )
0112     thr = xMin(fid)+xStep(fid)*thr;
0113     child(k)=K; fids(k)=fid-1; thrs(k)=thr;
0114     wtsAll0{K}=wts0.*left0; wtsAll0{K+1}=wts0.*~left0;
0115     wtsAll1{K}=wts1.*left1; wtsAll1{K+1}=wts1.*~left1;
0116     depth(K:K+1)=depth(k)+1; K=K+2;
0117   <span class="keyword">end</span>; k=k+1;
0118 <span class="keyword">end</span>; K=K-1;
0119 
0120 <span class="comment">% create output model struct</span>
0121 tree=struct(<span class="string">'fids'</span>,fids(1:K),<span class="string">'thrs'</span>,thrs(1:K),<span class="string">'child'</span>,child(1:K),<span class="keyword">...</span>
0122   <span class="string">'hs'</span>,hs(1:K),<span class="string">'weights'</span>,weights(1:K),<span class="string">'depth'</span>,depth(1:K));
0123 <span class="keyword">if</span>(nargout&gt;=3), err=sum(errs(1:K).*tree.weights.*(tree.child==0)); <span class="keyword">end</span>
0124 
0125 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 05-May-2022 15:20:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>