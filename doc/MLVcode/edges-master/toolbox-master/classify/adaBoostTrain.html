<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of adaBoostTrain</title>
  <meta name="keywords" content="adaBoostTrain">
  <meta name="description" content="Train boosted decision tree classifier.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../../index.html">Home</a> &gt;  <a href="../../../index.html">MLVcode</a> &gt; <a href="../../index.html">edges-master</a> &gt; <a href="#">toolbox-master</a> &gt; <a href="index.html">classify</a> &gt; adaBoostTrain.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../../index.html"><img alt="<" border="0" src="../../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for MLVcode\edges-master\toolbox-master\classify&nbsp;<img alt=">" border="0" src="../../../../right.png"></a></td></tr></table>-->

<h1>adaBoostTrain
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>Train boosted decision tree classifier.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>function model = adaBoostTrain( X0, X1, varargin ) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Train boosted decision tree classifier.

 Heavily optimized code for training Discrete or Real AdaBoost where the
 weak classifiers are decision trees. With multi-core support enabled (see
 binaryTreeTrain.m), boosting 256 depth-2 trees over 5,000 features and
 5,000 data points takes under 5 seconds, see example below. Most of the
 training time is spent in binaryTreeTrain.m.

 For more information on how to quickly boost decision trees see:
   [1] R. Appel, T. Fuchs, P. Dollár, P. Perona; &quot;Quickly Boosting
   Decision Trees – Pruning Underachieving Features Early,&quot; ICML 2013.
 The code here implements a simple brute-force strategy with the option to
 sample features used for training each node for additional speedups.
 Further gains using the ideas from the ICML paper are possible. If you
 use this code please consider citing our ICML paper.

 USAGE
  model = adaBoostTrain( X0, X1, [pBoost] )

 INPUTS
  X0         - [N0xF] negative feature vectors
  X1         - [N1xF] positive feature vectors
  pBoost     - additional params (struct or name/value pairs)
   .pTree      - ['REQ'] parameters for binaryTreeTrain
   .nWeak      - [128] number of trees to learn
   .discrete   - [1] train Discrete-AdaBoost or Real-AdaBoost
   .verbose    - [0] if true print status information

 OUTPUTS
  model      - learned boosted tree classifier w the following fields
   .fids       - [K x nWeak] feature ids for each node
   .thrs       - [K x nWeak] threshold corresponding to each fid
   .child      - [K x nWeak] index of child for each node (1-indexed)
   .hs         - [K x nWeak] log ratio (.5*log(p/(1-p)) at each node
   .weights    - [K x nWeak] total sample weight at each node
   .depth      - [K x nWeak] depth of each node
   .errs       - [1 x nWeak] error for each tree (for debugging)
   .losses     - [1 x nWeak] loss after every iteration (for debugging)
   .treeDepth  - depth of all leaf nodes (or 0 if leaf depth varies)

 EXAMPLE
  % output should be: 'Testing err=0.0145 fp=0.0165 fn=0.0125'
  N=5000; F=5000; sep=.01; RandStream.getGlobalStream.reset();
  [xTrn,hTrn,xTst,hTst]=demoGenData(N,N,2,F/10,sep,.5,0);
  xTrn=repmat(single(xTrn),[1 10]); xTst=repmat(single(xTst),[1 10]);
  pBoost=struct('nWeak',256,'verbose',16,'pTree',struct('maxDepth',2));
  model = adaBoostTrain( xTrn(hTrn==1,:), xTrn(hTrn==2,:), pBoost );
  fp = mean(adaBoostApply( xTst(hTst==1,:), model )&gt;0);
  fn = mean(adaBoostApply( xTst(hTst==2,:), model )&lt;0);
  fprintf('Testing err=%.4f fp=%.4f fn=%.4f\n',(fp+fn)/2,fp,fn);

 See also <a href="adaBoostApply.html" class="code" title="function hs = adaBoostApply( X, model, maxDepth, minWeight, nThreads )">adaBoostApply</a>, <a href="binaryTreeTrain.html" class="code" title="function [tree,data,err] = binaryTreeTrain( data, varargin )">binaryTreeTrain</a>, <a href="demoGenData.html" class="code" title="function [X0,H0,X1,H1] = demoGenData1(n0,n1,k,d,sep,ecc,frc)">demoGenData</a>

 Piotr's Computer Vision Matlab Toolbox      Version 3.21
 Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]
 Licensed under the Simplified BSD License [see external/bsd.txt]</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
<li><a href="binaryTreeApply.html" class="code" title="function hs = binaryTreeApply( X, tree, maxDepth, minWeight, nThreads )">binaryTreeApply</a>	Apply learned binary decision tree classifier.</li><li><a href="binaryTreeTrain.html" class="code" title="function [tree,data,err] = binaryTreeTrain( data, varargin )">binaryTreeTrain</a>	Train binary decision tree classifier.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = adaBoostTrain( X0, X1, varargin )</a>
0002 <span class="comment">% Train boosted decision tree classifier.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Heavily optimized code for training Discrete or Real AdaBoost where the</span>
0005 <span class="comment">% weak classifiers are decision trees. With multi-core support enabled (see</span>
0006 <span class="comment">% binaryTreeTrain.m), boosting 256 depth-2 trees over 5,000 features and</span>
0007 <span class="comment">% 5,000 data points takes under 5 seconds, see example below. Most of the</span>
0008 <span class="comment">% training time is spent in binaryTreeTrain.m.</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% For more information on how to quickly boost decision trees see:</span>
0011 <span class="comment">%   [1] R. Appel, T. Fuchs, P. Dollár, P. Perona; &quot;Quickly Boosting</span>
0012 <span class="comment">%   Decision Trees – Pruning Underachieving Features Early,&quot; ICML 2013.</span>
0013 <span class="comment">% The code here implements a simple brute-force strategy with the option to</span>
0014 <span class="comment">% sample features used for training each node for additional speedups.</span>
0015 <span class="comment">% Further gains using the ideas from the ICML paper are possible. If you</span>
0016 <span class="comment">% use this code please consider citing our ICML paper.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% USAGE</span>
0019 <span class="comment">%  model = adaBoostTrain( X0, X1, [pBoost] )</span>
0020 <span class="comment">%</span>
0021 <span class="comment">% INPUTS</span>
0022 <span class="comment">%  X0         - [N0xF] negative feature vectors</span>
0023 <span class="comment">%  X1         - [N1xF] positive feature vectors</span>
0024 <span class="comment">%  pBoost     - additional params (struct or name/value pairs)</span>
0025 <span class="comment">%   .pTree      - ['REQ'] parameters for binaryTreeTrain</span>
0026 <span class="comment">%   .nWeak      - [128] number of trees to learn</span>
0027 <span class="comment">%   .discrete   - [1] train Discrete-AdaBoost or Real-AdaBoost</span>
0028 <span class="comment">%   .verbose    - [0] if true print status information</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% OUTPUTS</span>
0031 <span class="comment">%  model      - learned boosted tree classifier w the following fields</span>
0032 <span class="comment">%   .fids       - [K x nWeak] feature ids for each node</span>
0033 <span class="comment">%   .thrs       - [K x nWeak] threshold corresponding to each fid</span>
0034 <span class="comment">%   .child      - [K x nWeak] index of child for each node (1-indexed)</span>
0035 <span class="comment">%   .hs         - [K x nWeak] log ratio (.5*log(p/(1-p)) at each node</span>
0036 <span class="comment">%   .weights    - [K x nWeak] total sample weight at each node</span>
0037 <span class="comment">%   .depth      - [K x nWeak] depth of each node</span>
0038 <span class="comment">%   .errs       - [1 x nWeak] error for each tree (for debugging)</span>
0039 <span class="comment">%   .losses     - [1 x nWeak] loss after every iteration (for debugging)</span>
0040 <span class="comment">%   .treeDepth  - depth of all leaf nodes (or 0 if leaf depth varies)</span>
0041 <span class="comment">%</span>
0042 <span class="comment">% EXAMPLE</span>
0043 <span class="comment">%  % output should be: 'Testing err=0.0145 fp=0.0165 fn=0.0125'</span>
0044 <span class="comment">%  N=5000; F=5000; sep=.01; RandStream.getGlobalStream.reset();</span>
0045 <span class="comment">%  [xTrn,hTrn,xTst,hTst]=demoGenData(N,N,2,F/10,sep,.5,0);</span>
0046 <span class="comment">%  xTrn=repmat(single(xTrn),[1 10]); xTst=repmat(single(xTst),[1 10]);</span>
0047 <span class="comment">%  pBoost=struct('nWeak',256,'verbose',16,'pTree',struct('maxDepth',2));</span>
0048 <span class="comment">%  model = adaBoostTrain( xTrn(hTrn==1,:), xTrn(hTrn==2,:), pBoost );</span>
0049 <span class="comment">%  fp = mean(adaBoostApply( xTst(hTst==1,:), model )&gt;0);</span>
0050 <span class="comment">%  fn = mean(adaBoostApply( xTst(hTst==2,:), model )&lt;0);</span>
0051 <span class="comment">%  fprintf('Testing err=%.4f fp=%.4f fn=%.4f\n',(fp+fn)/2,fp,fn);</span>
0052 <span class="comment">%</span>
0053 <span class="comment">% See also adaBoostApply, binaryTreeTrain, demoGenData</span>
0054 <span class="comment">%</span>
0055 <span class="comment">% Piotr's Computer Vision Matlab Toolbox      Version 3.21</span>
0056 <span class="comment">% Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]</span>
0057 <span class="comment">% Licensed under the Simplified BSD License [see external/bsd.txt]</span>
0058 
0059 <span class="comment">% get additional parameters</span>
0060 dfs={ <span class="string">'pTree'</span>,<span class="string">'REQ'</span>, <span class="string">'nWeak'</span>,128, <span class="string">'discrete'</span>,1, <span class="string">'verbose'</span>,0 };
0061 [pTree,nWeak,discrete,verbose]=getPrmDflt(varargin,dfs,1);
0062 nThreads=[]; <span class="keyword">if</span>(isfield(pTree,<span class="string">'nThreads'</span>)), nThreads=pTree.nThreads; <span class="keyword">end</span>
0063 
0064 <span class="comment">% main loop</span>
0065 [N0,F]=size(X0); [N1,F1]=size(X1); assert(F==F1);
0066 msg=<span class="string">'Training AdaBoost: nWeak=%3i nFtrs=%i pos=%i neg=%i\n'</span>;
0067 <span class="keyword">if</span>(verbose), fprintf(msg,nWeak,F,N1,N0); start=clock; <span class="keyword">end</span>
0068 data=struct(<span class="string">'X0'</span>,X0,<span class="string">'X1'</span>,X1);
0069 H0=zeros(N0,1); H1=zeros(N1,1);
0070 losses=zeros(1,nWeak); errs=losses;
0071 <span class="keyword">for</span> i=1:nWeak
0072   <span class="comment">% train tree and classify each example</span>
0073   [tree,data,err]=<a href="binaryTreeTrain.html" class="code" title="function [tree,data,err] = binaryTreeTrain( data, varargin )">binaryTreeTrain</a>(data,pTree);
0074   <span class="keyword">if</span>(discrete), tree.hs=(tree.hs&gt;0)*2-1; <span class="keyword">end</span>
0075   h0 = <a href="binaryTreeApply.html" class="code" title="function hs = binaryTreeApply( X, tree, maxDepth, minWeight, nThreads )">binaryTreeApply</a>(X0,tree,[],[],nThreads);
0076   h1 = <a href="binaryTreeApply.html" class="code" title="function hs = binaryTreeApply( X, tree, maxDepth, minWeight, nThreads )">binaryTreeApply</a>(X1,tree,[],[],nThreads);
0077   <span class="comment">% compute alpha and incorporate directly into tree model</span>
0078   alpha=1; <span class="keyword">if</span>(discrete), alpha=max(-5,min(5,.5*log((1-err)/err))); <span class="keyword">end</span>
0079   <span class="keyword">if</span>(verbose &amp;&amp; alpha&lt;=0), nWeak=i-1; disp(<span class="string">' stopping early'</span>); <span class="keyword">break</span>; <span class="keyword">end</span>
0080   tree.hs=tree.hs*alpha;
0081   <span class="comment">% update cumulative scores H and weights</span>
0082   H0=H0+h0*alpha; data.wts0=exp( H0)/N0/2;
0083   H1=H1+h1*alpha; data.wts1=exp(-H1)/N1/2;
0084   loss=sum(data.wts0)+sum(data.wts1);
0085   <span class="keyword">if</span>(i==1), trees=repmat(tree,nWeak,1); <span class="keyword">end</span>
0086   trees(i)=tree; errs(i)=err; losses(i)=loss;
0087   msg=<span class="string">' i=%4i alpha=%.3f err=%.3f loss=%.2e\n'</span>;
0088   <span class="keyword">if</span>(mod(i,verbose)==0), fprintf(msg,i,alpha,err,loss); <span class="keyword">end</span>
0089   <span class="keyword">if</span>(verbose &amp;&amp; loss&lt;1e-40), nWeak=i; disp(<span class="string">' stopping early'</span>); <span class="keyword">break</span>; <span class="keyword">end</span>
0090 <span class="keyword">end</span>
0091 
0092 <span class="comment">% create output model struct</span>
0093 k=0; <span class="keyword">for</span> i=1:nWeak, k=max(k,size(trees(i).fids,1)); <span class="keyword">end</span>
0094 Z = @(type) zeros(k,nWeak,type);
0095 model=struct( <span class="string">'fids'</span>,Z(<span class="string">'uint32'</span>), <span class="string">'thrs'</span>,Z(data.xType), <span class="keyword">...</span>
0096   <span class="string">'child'</span>,Z(<span class="string">'uint32'</span>), <span class="string">'hs'</span>,Z(<span class="string">'single'</span>), <span class="string">'weights'</span>,Z(<span class="string">'single'</span>), <span class="keyword">...</span>
0097   <span class="string">'depth'</span>,Z(<span class="string">'uint32'</span>), <span class="string">'errs'</span>,errs, <span class="string">'losses'</span>,losses );
0098 <span class="keyword">for</span> i=1:nWeak, T=trees(i); k=size(T.fids,1);
0099   model.fids(1:k,i)=T.fids; model.thrs(1:k,i)=T.thrs;
0100   model.child(1:k,i)=T.child; model.hs(1:k,i)=T.hs;
0101   model.weights(1:k,i)=T.weights; model.depth(1:k,i)=T.depth;
0102 <span class="keyword">end</span>
0103 depth = max(model.depth(:));
0104 model.treeDepth = depth * uint32(all(model.depth(~model.child)==depth));
0105 
0106 <span class="comment">% output info to log</span>
0107 msg=<span class="string">'Done training err=%.4f fp=%.4f fn=%.4f (t=%.1fs).\n'</span>;
0108 <span class="keyword">if</span>(verbose), fp=mean(H0&gt;0); fn=mean(H1&lt;0);
0109   fprintf(msg,(fp+fn)/2,fp,fn,etime(clock,start)); <span class="keyword">end</span>
0110 
0111 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 05-May-2022 14:52:24 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>