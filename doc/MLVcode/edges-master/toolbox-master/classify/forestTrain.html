<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of forestTrain</title>
  <meta name="keywords" content="forestTrain">
  <meta name="description" content="Train random forest classifier.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../../index.html">Home</a> &gt;  <a href="../../../index.html">MLVcode</a> &gt; <a href="../../index.html">edges-master</a> &gt; <a href="#">toolbox-master</a> &gt; <a href="index.html">classify</a> &gt; forestTrain.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../../index.html"><img alt="<" border="0" src="../../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for MLVcode\edges-master\toolbox-master\classify&nbsp;<img alt=">" border="0" src="../../../../right.png"></a></td></tr></table>-->

<h1>forestTrain
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>Train random forest classifier.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>function forest = forestTrain( data, hs, varargin ) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Train random forest classifier.

 Dimensions:
  M - number trees
  F - number features
  N - number input vectors
  H - number classes

 USAGE
  forest = forestTrain( data, hs, [varargin] )

 INPUTS
  data     - [NxF] N length F feature vectors
  hs       - [Nx1] or {Nx1} target output labels in [1,H]
  varargin - additional params (struct or name/value pairs)
   .M          - [1] number of trees to train
   .H          - [max(hs)] number of classes
   .N1         - [5*N/M] number of data points for training each tree
   .F1         - [sqrt(F)] number features to sample for each node split
   .split      - ['gini'] options include 'gini', 'entropy' and 'twoing'
   .minCount   - [1] minimum number of data points to allow split
   .minChild   - [1] minimum number of data points allowed at child nodes
   .maxDepth   - [64] maximum depth of tree
   .dWts       - [] weights used for sampling and weighing each data point
   .fWts       - [] weights used for sampling features
   .discretize - [] optional function mapping structured to class labels
                    format: [hsClass,hBest] = discretize(hsStructured,H);

 OUTPUTS
  forest   - learned forest model struct array w the following fields
   .fids     - [Kx1] feature ids for each node
   .thrs     - [Kx1] threshold corresponding to each fid
   .child    - [Kx1] index of child for each node
   .distr    - [KxH] prob distribution at each node
   .hs       - [Kx1] or {Kx1} most likely label at each node
   .count    - [Kx1] number of data points at each node
   .depth    - [Kx1] depth of each node

 EXAMPLE
  N=10000; H=5; d=2; [xs0,hs0,xs1,hs1]=demoGenData(N,N,H,d,1,1);
  xs0=single(xs0); xs1=single(xs1);
  pTrain={'maxDepth',50,'F1',2,'M',150,'minChild',5};
  tic, forest=forestTrain(xs0,hs0,pTrain{:}); toc
  hsPr0 = forestApply(xs0,forest);
  hsPr1 = forestApply(xs1,forest);
  e0=mean(hsPr0~=hs0); e1=mean(hsPr1~=hs1);
  fprintf('errors trn=%f tst=%f\n',e0,e1); figure(1);
  subplot(2,2,1); visualizeData(xs0,2,hs0);
  subplot(2,2,2); visualizeData(xs0,2,hsPr0);
  subplot(2,2,3); visualizeData(xs1,2,hs1);
  subplot(2,2,4); visualizeData(xs1,2,hsPr1);

 See also <a href="forestApply.html" class="code" title="function [hs,ps] = forestApply( data, forest, maxDepth, minCount, best )">forestApply</a>, <a href="fernsClfTrain.html" class="code" title="function [ferns,hsPr] = fernsClfTrain( data, hs, varargin )">fernsClfTrain</a>

 Piotr's Computer Vision Matlab Toolbox      Version 3.24
 Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]
 Licensed under the Simplified BSD License [see external/bsd.txt]</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function tree = treeTrain( data, hs, dWts, prmTree )</a></li><li><a href="#_sub2" class="code">function ids = wswor( prob, N, trials )</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function forest = forestTrain( data, hs, varargin )</a>
0002 <span class="comment">% Train random forest classifier.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Dimensions:</span>
0005 <span class="comment">%  M - number trees</span>
0006 <span class="comment">%  F - number features</span>
0007 <span class="comment">%  N - number input vectors</span>
0008 <span class="comment">%  H - number classes</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% USAGE</span>
0011 <span class="comment">%  forest = forestTrain( data, hs, [varargin] )</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% INPUTS</span>
0014 <span class="comment">%  data     - [NxF] N length F feature vectors</span>
0015 <span class="comment">%  hs       - [Nx1] or {Nx1} target output labels in [1,H]</span>
0016 <span class="comment">%  varargin - additional params (struct or name/value pairs)</span>
0017 <span class="comment">%   .M          - [1] number of trees to train</span>
0018 <span class="comment">%   .H          - [max(hs)] number of classes</span>
0019 <span class="comment">%   .N1         - [5*N/M] number of data points for training each tree</span>
0020 <span class="comment">%   .F1         - [sqrt(F)] number features to sample for each node split</span>
0021 <span class="comment">%   .split      - ['gini'] options include 'gini', 'entropy' and 'twoing'</span>
0022 <span class="comment">%   .minCount   - [1] minimum number of data points to allow split</span>
0023 <span class="comment">%   .minChild   - [1] minimum number of data points allowed at child nodes</span>
0024 <span class="comment">%   .maxDepth   - [64] maximum depth of tree</span>
0025 <span class="comment">%   .dWts       - [] weights used for sampling and weighing each data point</span>
0026 <span class="comment">%   .fWts       - [] weights used for sampling features</span>
0027 <span class="comment">%   .discretize - [] optional function mapping structured to class labels</span>
0028 <span class="comment">%                    format: [hsClass,hBest] = discretize(hsStructured,H);</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% OUTPUTS</span>
0031 <span class="comment">%  forest   - learned forest model struct array w the following fields</span>
0032 <span class="comment">%   .fids     - [Kx1] feature ids for each node</span>
0033 <span class="comment">%   .thrs     - [Kx1] threshold corresponding to each fid</span>
0034 <span class="comment">%   .child    - [Kx1] index of child for each node</span>
0035 <span class="comment">%   .distr    - [KxH] prob distribution at each node</span>
0036 <span class="comment">%   .hs       - [Kx1] or {Kx1} most likely label at each node</span>
0037 <span class="comment">%   .count    - [Kx1] number of data points at each node</span>
0038 <span class="comment">%   .depth    - [Kx1] depth of each node</span>
0039 <span class="comment">%</span>
0040 <span class="comment">% EXAMPLE</span>
0041 <span class="comment">%  N=10000; H=5; d=2; [xs0,hs0,xs1,hs1]=demoGenData(N,N,H,d,1,1);</span>
0042 <span class="comment">%  xs0=single(xs0); xs1=single(xs1);</span>
0043 <span class="comment">%  pTrain={'maxDepth',50,'F1',2,'M',150,'minChild',5};</span>
0044 <span class="comment">%  tic, forest=forestTrain(xs0,hs0,pTrain{:}); toc</span>
0045 <span class="comment">%  hsPr0 = forestApply(xs0,forest);</span>
0046 <span class="comment">%  hsPr1 = forestApply(xs1,forest);</span>
0047 <span class="comment">%  e0=mean(hsPr0~=hs0); e1=mean(hsPr1~=hs1);</span>
0048 <span class="comment">%  fprintf('errors trn=%f tst=%f\n',e0,e1); figure(1);</span>
0049 <span class="comment">%  subplot(2,2,1); visualizeData(xs0,2,hs0);</span>
0050 <span class="comment">%  subplot(2,2,2); visualizeData(xs0,2,hsPr0);</span>
0051 <span class="comment">%  subplot(2,2,3); visualizeData(xs1,2,hs1);</span>
0052 <span class="comment">%  subplot(2,2,4); visualizeData(xs1,2,hsPr1);</span>
0053 <span class="comment">%</span>
0054 <span class="comment">% See also forestApply, fernsClfTrain</span>
0055 <span class="comment">%</span>
0056 <span class="comment">% Piotr's Computer Vision Matlab Toolbox      Version 3.24</span>
0057 <span class="comment">% Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]</span>
0058 <span class="comment">% Licensed under the Simplified BSD License [see external/bsd.txt]</span>
0059 
0060 <span class="comment">% get additional parameters and fill in remaining parameters</span>
0061 dfs={ <span class="string">'M'</span>,1, <span class="string">'H'</span>,[], <span class="string">'N1'</span>,[], <span class="string">'F1'</span>,[], <span class="string">'split'</span>,<span class="string">'gini'</span>, <span class="string">'minCount'</span>,1, <span class="keyword">...</span>
0062   <span class="string">'minChild'</span>,1, <span class="string">'maxDepth'</span>,64, <span class="string">'dWts'</span>,[], <span class="string">'fWts'</span>,[], <span class="string">'discretize'</span>,<span class="string">''</span> };
0063 [M,H,N1,F1,splitStr,minCount,minChild,maxDepth,dWts,fWts,discretize] = <span class="keyword">...</span>
0064   getPrmDflt(varargin,dfs,1);
0065 [N,F]=size(data); assert(length(hs)==N); discr=~isempty(discretize);
0066 minChild=max(1,minChild); minCount=max([1 minCount minChild]);
0067 <span class="keyword">if</span>(isempty(H)), H=max(hs); <span class="keyword">end</span>; assert(discr || all(hs&gt;0 &amp; hs&lt;=H));
0068 <span class="keyword">if</span>(isempty(N1)), N1=round(5*N/M); <span class="keyword">end</span>; N1=min(N,N1);
0069 <span class="keyword">if</span>(isempty(F1)), F1=round(sqrt(F)); <span class="keyword">end</span>; F1=min(F,F1);
0070 <span class="keyword">if</span>(isempty(dWts)), dWts=ones(1,N,<span class="string">'single'</span>); <span class="keyword">end</span>; dWts=dWts/sum(dWts);
0071 <span class="keyword">if</span>(isempty(fWts)), fWts=ones(1,F,<span class="string">'single'</span>); <span class="keyword">end</span>; fWts=fWts/sum(fWts);
0072 split=find(strcmpi(splitStr,{<span class="string">'gini'</span>,<span class="string">'entropy'</span>,<span class="string">'twoing'</span>}))-1;
0073 <span class="keyword">if</span>(isempty(split)), error(<span class="string">'unknown splitting criteria: %s'</span>,splitStr); <span class="keyword">end</span>
0074 
0075 <span class="comment">% make sure data has correct types</span>
0076 <span class="keyword">if</span>(~isa(data,<span class="string">'single'</span>)), data=single(data); <span class="keyword">end</span>
0077 <span class="keyword">if</span>(~isa(hs,<span class="string">'uint32'</span>) &amp;&amp; ~discr), hs=uint32(hs); <span class="keyword">end</span>
0078 <span class="keyword">if</span>(~isa(fWts,<span class="string">'single'</span>)), fWts=single(fWts); <span class="keyword">end</span>
0079 <span class="keyword">if</span>(~isa(dWts,<span class="string">'single'</span>)), dWts=single(dWts); <span class="keyword">end</span>
0080 
0081 <span class="comment">% train M random trees on different subsets of data</span>
0082 prmTree = {H,F1,minCount,minChild,maxDepth,fWts,split,discretize};
0083 <span class="keyword">for</span> i=1:M
0084   <span class="keyword">if</span>(N==N1), data1=data; hs1=hs; dWts1=dWts; <span class="keyword">else</span>
0085     d=<a href="#_sub2" class="code" title="subfunction ids = wswor( prob, N, trials )">wswor</a>(dWts,N1,4); data1=data(d,:); hs1=hs(d);
0086     dWts1=dWts(d); dWts1=dWts1/sum(dWts1);
0087   <span class="keyword">end</span>
0088   tree = <a href="#_sub1" class="code" title="subfunction tree = treeTrain( data, hs, dWts, prmTree )">treeTrain</a>(data1,hs1,dWts1,prmTree);
0089   <span class="keyword">if</span>(i==1), forest=tree(ones(M,1)); <span class="keyword">else</span> forest(i)=tree; <span class="keyword">end</span>
0090 <span class="keyword">end</span>
0091 
0092 <span class="keyword">end</span>
0093 
0094 <a name="_sub1" href="#_subfunctions" class="code">function tree = treeTrain( data, hs, dWts, prmTree )</a>
0095 <span class="comment">% Train single random tree.</span>
0096 [H,F1,minCount,minChild,maxDepth,fWts,split,discretize]=deal(prmTree{:});
0097 N=size(data,1); K=2*N-1; discr=~isempty(discretize);
0098 thrs=zeros(K,1,<span class="string">'single'</span>); distr=zeros(K,H,<span class="string">'single'</span>);
0099 fids=zeros(K,1,<span class="string">'uint32'</span>); child=fids; count=fids; depth=fids;
0100 hsn=cell(K,1); dids=cell(K,1); dids{1}=uint32(1:N); k=1; K=2;
0101 <span class="keyword">while</span>( k &lt; K )
0102   <span class="comment">% get node data and store distribution</span>
0103   dids1=dids{k}; dids{k}=[]; hs1=hs(dids1); n1=length(hs1); count(k)=n1;
0104   <span class="keyword">if</span>(discr), [hs1,hsn{k}]=feval(discretize,hs1,H); hs1=uint32(hs1); <span class="keyword">end</span>
0105   <span class="keyword">if</span>(discr), assert(all(hs1&gt;0 &amp; hs1&lt;=H)); <span class="keyword">end</span>; pure=all(hs1(1)==hs1);
0106   <span class="keyword">if</span>(~discr), <span class="keyword">if</span>(pure), distr(k,hs1(1))=1; hsn{k}=hs1(1); <span class="keyword">else</span>
0107       distr(k,:)=histc(hs1,1:H)/n1; [~,hsn{k}]=max(distr(k,:)); <span class="keyword">end</span>; <span class="keyword">end</span>
0108   <span class="comment">% if pure node or insufficient data don't train split</span>
0109   <span class="keyword">if</span>( pure || n1&lt;=minCount || depth(k)&gt;maxDepth ), k=k+1; <span class="keyword">continue</span>; <span class="keyword">end</span>
0110   <span class="comment">% train split and continue</span>
0111   fids1=<a href="#_sub2" class="code" title="subfunction ids = wswor( prob, N, trials )">wswor</a>(fWts,F1,4); data1=data(dids1,fids1);
0112   [~,order1]=sort(data1); order1=uint32(order1-1);
0113   [fid,thr,gain]=forestFindThr(data1,hs1,dWts(dids1),order1,H,split);
0114   fid=fids1(fid); left=data(dids1,fid)&lt;thr; count0=nnz(left);
0115   <span class="keyword">if</span>( gain&gt;1e-10 &amp;&amp; count0&gt;=minChild &amp;&amp; (n1-count0)&gt;=minChild )
0116     child(k)=K; fids(k)=fid-1; thrs(k)=thr;
0117     dids{K}=dids1(left); dids{K+1}=dids1(~left);
0118     depth(K:K+1)=depth(k)+1; K=K+2;
0119   <span class="keyword">end</span>; k=k+1;
0120 <span class="keyword">end</span>
0121 <span class="comment">% create output model struct</span>
0122 K=1:K-1; <span class="keyword">if</span>(discr), hsn={hsn(K)}; <span class="keyword">else</span> hsn=[hsn{K}]'; <span class="keyword">end</span>
0123 tree=struct(<span class="string">'fids'</span>,fids(K),<span class="string">'thrs'</span>,thrs(K),<span class="string">'child'</span>,child(K),<span class="keyword">...</span>
0124   <span class="string">'distr'</span>,distr(K,:),<span class="string">'hs'</span>,hsn,<span class="string">'count'</span>,count(K),<span class="string">'depth'</span>,depth(K));
0125 <span class="keyword">end</span>
0126 
0127 <a name="_sub2" href="#_subfunctions" class="code">function ids = wswor( prob, N, trials )</a>
0128 <span class="comment">% Fast weighted sample without replacement. Alternative to:</span>
0129 <span class="comment">%  ids=datasample(1:length(prob),N,'weights',prob,'replace',false);</span>
0130 M=length(prob); assert(N&lt;=M); <span class="keyword">if</span>(N==M), ids=1:N; <span class="keyword">return</span>; <span class="keyword">end</span>
0131 <span class="keyword">if</span>(all(prob(1)==prob)), ids=randperm(M,N); <span class="keyword">return</span>; <span class="keyword">end</span>
0132 cumprob=min([0 cumsum(prob)],1); assert(abs(cumprob(end)-1)&lt;.01);
0133 cumprob(end)=1; [~,ids]=histc(rand(N*trials,1),cumprob);
0134 [s,ord]=sort(ids); K(ord)=[1; diff(s)]~=0; ids=ids(K);
0135 <span class="keyword">if</span>(length(ids)&lt;N), ids=<a href="#_sub2" class="code" title="subfunction ids = wswor( prob, N, trials )">wswor</a>(cumprob,N,trials*2); <span class="keyword">end</span>
0136 ids=ids(1:N)';
0137 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 05-May-2022 15:20:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>