<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of softMin</title>
  <meta name="keywords" content="softMin">
  <meta name="description" content="Calculates the softMin of a vector.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../../index.html">Home</a> &gt;  <a href="../../../index.html">MLVcode</a> &gt; <a href="../../index.html">edges-master</a> &gt; <a href="#">toolbox-master</a> &gt; <a href="index.html">classify</a> &gt; softMin.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../../index.html"><img alt="<" border="0" src="../../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for MLVcode\edges-master\toolbox-master\classify&nbsp;<img alt=">" border="0" src="../../../../right.png"></a></td></tr></table>-->

<h1>softMin
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>Calculates the softMin of a vector.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="box"><strong>function M = softMin( D, sigma ) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Calculates the softMin of a vector.

 Let D be a vector.  Then the softMin of D is defined as:
   s = exp(-D/sigma^2) / sum( exp(-D/sigma^2) )
 The softMin is a way of taking a dissimilarity (distance) vector D and
 converting it to a similarity vector s, such that sum(s)==1. If D is an
 NxK array, is is treated as N K-dimensional vectors, and the return is
 likewise an NxK array.  This is useful if D is a distance matrix,
 generated by the likes of pdist2.

 Note that as sigma-&gt;0, softMin's behavior tends toward that of the
 standard min function.  That is the softMin of a vector D has all zeros
 with a single 1 in the location of the smallest value of D. For example,
 &quot;softMin([.2 .4 .1 .3],eps)&quot; returns &quot;[0 0 1 0]&quot;.  As sigma-&gt;inf, then
 softMin(D,sigma) tends toward &quot;ones(1,n)/n&quot;, where n==length(D).

 If D contains the squared euclidean distance between a point y and k
 points xi, then there is a probabilistic interpretation for softMin.  If
 we think of the k points representing equal variant gaussians each with
 mean xi and std sigma, then the softMin returns the relative probability
 of y being generated by each gaussian.

 USAGE
  M = softMin( D, sigma )

 INPUTS
  D       - NxK dissimilarity matrix
  sigma   - controls 'softness' of softMin

 OUTPUTS
  M       - the softMin (indexes into D)

 EXAMPLE - 1
  C = [0 0; 1 0; 0 1; 1 1]; x=[.7,.3; .1 .2];
  D = pdist2( x, C ), M = softMin( D, .25 )

 EXAMPLE - 2
  fplot( 'softMin( [0.5 0.2 .4], x )', [0 5] );
  xlabel('sigma'); ylabel('assignments')

 See also <a href="pdist2.html" class="code" title="function D = pdist2( X, Y, metric )">PDIST2</a>, SOFTMAX

 Piotr's Computer Vision Matlab Toolbox      Version 2.0
 Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]
 Licensed under the Simplified BSD License [see external/bsd.txt]</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../../../../matlabicon.gif)">
</ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function M = softMin( D, sigma )</a>
0002 <span class="comment">% Calculates the softMin of a vector.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% Let D be a vector.  Then the softMin of D is defined as:</span>
0005 <span class="comment">%   s = exp(-D/sigma^2) / sum( exp(-D/sigma^2) )</span>
0006 <span class="comment">% The softMin is a way of taking a dissimilarity (distance) vector D and</span>
0007 <span class="comment">% converting it to a similarity vector s, such that sum(s)==1. If D is an</span>
0008 <span class="comment">% NxK array, is is treated as N K-dimensional vectors, and the return is</span>
0009 <span class="comment">% likewise an NxK array.  This is useful if D is a distance matrix,</span>
0010 <span class="comment">% generated by the likes of pdist2.</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% Note that as sigma-&gt;0, softMin's behavior tends toward that of the</span>
0013 <span class="comment">% standard min function.  That is the softMin of a vector D has all zeros</span>
0014 <span class="comment">% with a single 1 in the location of the smallest value of D. For example,</span>
0015 <span class="comment">% &quot;softMin([.2 .4 .1 .3],eps)&quot; returns &quot;[0 0 1 0]&quot;.  As sigma-&gt;inf, then</span>
0016 <span class="comment">% softMin(D,sigma) tends toward &quot;ones(1,n)/n&quot;, where n==length(D).</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% If D contains the squared euclidean distance between a point y and k</span>
0019 <span class="comment">% points xi, then there is a probabilistic interpretation for softMin.  If</span>
0020 <span class="comment">% we think of the k points representing equal variant gaussians each with</span>
0021 <span class="comment">% mean xi and std sigma, then the softMin returns the relative probability</span>
0022 <span class="comment">% of y being generated by each gaussian.</span>
0023 <span class="comment">%</span>
0024 <span class="comment">% USAGE</span>
0025 <span class="comment">%  M = softMin( D, sigma )</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% INPUTS</span>
0028 <span class="comment">%  D       - NxK dissimilarity matrix</span>
0029 <span class="comment">%  sigma   - controls 'softness' of softMin</span>
0030 <span class="comment">%</span>
0031 <span class="comment">% OUTPUTS</span>
0032 <span class="comment">%  M       - the softMin (indexes into D)</span>
0033 <span class="comment">%</span>
0034 <span class="comment">% EXAMPLE - 1</span>
0035 <span class="comment">%  C = [0 0; 1 0; 0 1; 1 1]; x=[.7,.3; .1 .2];</span>
0036 <span class="comment">%  D = pdist2( x, C ), M = softMin( D, .25 )</span>
0037 <span class="comment">%</span>
0038 <span class="comment">% EXAMPLE - 2</span>
0039 <span class="comment">%  fplot( 'softMin( [0.5 0.2 .4], x )', [0 5] );</span>
0040 <span class="comment">%  xlabel('sigma'); ylabel('assignments')</span>
0041 <span class="comment">%</span>
0042 <span class="comment">% See also PDIST2, SOFTMAX</span>
0043 <span class="comment">%</span>
0044 <span class="comment">% Piotr's Computer Vision Matlab Toolbox      Version 2.0</span>
0045 <span class="comment">% Copyright 2014 Piotr Dollar.  [pdollar-at-gmail.com]</span>
0046 <span class="comment">% Licensed under the Simplified BSD License [see external/bsd.txt]</span>
0047 
0048 <span class="keyword">if</span>( sigma==0 ) <span class="comment">% special case, make fast</span>
0049   [~, inds] = min(D,[],2); [n, k] = size(D);
0050   M = subsToArray( [(1:n)' inds], ones(n,1), [n k] );
0051 
0052 <span class="keyword">else</span> <span class="comment">% general case</span>
0053   M = exp( -D / sigma^2 );
0054   M(isinf(M))=1e50;
0055   sumM = sum( M, 2 );
0056   sumMzero = (sumM==0);
0057   <span class="keyword">if</span>( any(sumMzero) )
0058     [~, inds] = min(D,[],2); [n, k] = size(D);
0059     Mhard = subsToArray( [(1:n)' inds], ones(n,1), [n k] );
0060     M( sumMzero, : ) = Mhard( sumMzero, : );
0061     sumM = sum( M, 2 );
0062   <span class="keyword">end</span>
0063   M = M ./ sumM( :, ones(1,size(M,2)) );
0064 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Thu 05-May-2022 15:20:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>